{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from funs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Locally\n",
    "# data_url = 'https://github.com/dmika1234/ml_uwr_22/blob/Project/Project/data/fake_job_postings.csv'\n",
    "data_path = 'data/fake_job_postings.csv'\n",
    "raw_data = pd.read_csv(data_path)\n",
    "\n",
    "# For colab\n",
    "# data_url = '/content/fake_job_postings.csv'\n",
    "# raw_data = pd.read_csv(data_url, error_bad_lines=False, engine=\"python\")\n",
    "#straszny problem miałem, żeby wczytać te dane tak ja ty to robiłeś. dziwne błędy mi wyskakiwały"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "d:\\Studia\\MachineLearning\\ml_uwr_22\\Project\\funs.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_data.fillna('', inplace=True)\n",
      "d:\\Studia\\MachineLearning\\ml_uwr_22\\Project\\funs.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_data.fillna('', inplace=True)\n",
      "d:\\Studia\\MachineLearning\\ml_uwr_22\\Project\\funs.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_data.fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "text_colnames = ['company_profile', 'description', 'requirements', 'benefits']\n",
    "DataPrep = DataPreprocessor()\n",
    "\n",
    "text_data_ls = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=list)\n",
    "text_data_np = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=np.array)\n",
    "text_data_str = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=join_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Word2vec playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Our data 1st try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec model on job description column\n",
    "model1 = Word2Vec(text_data_ls['description'], vector_size=20, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.4592648 ,  2.1658094 ,  3.445258  ,  6.449147  ,  2.7680726 ,\n",
       "        8.245474  ,  1.1446196 ,  4.5827475 ,  0.5369002 , -4.02162   ,\n",
       "        1.5656481 ,  4.4785323 ,  0.6097932 , -3.1391037 ,  2.5798745 ,\n",
       "       -3.9150026 ,  3.0776334 , -4.092833  , -0.14151247,  0.7096834 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv['growing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('expanding', 0.8615612983703613),\n",
       " ('unbeatable', 0.7613900899887085),\n",
       " ('500equinix', 0.7303839325904846),\n",
       " ('paced', 0.7217192649841309),\n",
       " ('boredommaintaining', 0.7178643941879272),\n",
       " ('areseeking', 0.7041919827461243),\n",
       " ('experiencedescriptiondo', 0.7030121088027954),\n",
       " ('wisebanyan', 0.6996685266494751),\n",
       " ('quotaassist', 0.696188747882843),\n",
       " ('url_8b2e90799ea4104211c3ad7a24eb3db2a7e5f5a4f6b9a53c4325b4499b1ff797',\n",
       "  0.6932835578918457)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(positive='growing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_word2vec(text, model, vector_size=20, agg_func=np.mean):\n",
    "    # Get the word2vec representation of each word in the text\n",
    "    word_vectors = np.array([model.wv[word] for word in text if word in model.wv.index_to_key])\n",
    "    res = agg_func(word_vectors, axis=0)\n",
    "    if np.isnan(res).any():\n",
    "        res = np.zeros(vector_size)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programy\\Anaconda3\\envs\\UniEnv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n"
     ]
    }
   ],
   "source": [
    "X_train_vectors = text_data_np['description'].apply(lambda x: get_agg_word2vec(x, model1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectors = np.array(X_train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Pretrained model on Google news data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Please note that this code needs only to be run in a fresh runtime.\n",
    "# # However, it can be rerun afterwards too.\n",
    "# !pip install -q gdown httpimport\n",
    "# ![ -e mnist.npz ] || gdown 'https://drive.google.com/uc?id=1QPaC3IKB_5tX6yIZgRgkpcqFrfVqPTXU' -O mnist.npz\n",
    "!pip install gdown\n",
    "gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_google = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('grown', 0.6805573105812073),\n",
       " ('burgeoning', 0.6775053143501282),\n",
       " ('grow', 0.6641595959663391),\n",
       " ('rapidly_expanding', 0.6622884273529053),\n",
       " ('increasing', 0.6527025699615479),\n",
       " ('grows', 0.6311837434768677),\n",
       " ('Growing', 0.6133630871772766),\n",
       " ('expanding', 0.5987905263900757),\n",
       " ('rising', 0.5733980536460876),\n",
       " ('growning', 0.5670080780982971)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_google.most_similar(positive='growing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_google.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00536227,  0.00236431,  0.0510335 ,  0.09009273, -0.0930295 ,\n",
       "       -0.07116809,  0.06458873,  0.08972988, -0.05015428, -0.03763372],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['this', 'is', 'a', 'good', 'example'], ['this', 'is', 'another', 'example']]\n",
    "model = Word2Vec(sentences, vector_size=10, min_count=1)\n",
    "model.wv['example']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'is', 'this', 'another', 'good', 'a']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=10)\n",
    "\n",
    "# Fit the vectorizer to the job description column\n",
    "X_train_tfidf = vectorizer.fit_transform(text_data_str['description'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fc72597e3ebe3b3cf4d5e7c9c96009b83c4c16e8afac821de511c3301abc35f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
