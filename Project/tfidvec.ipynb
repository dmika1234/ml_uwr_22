{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dmika1234/ml_uwr_22/blob/Project/Project/tfidvec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functions\n",
    "## Locally \n",
    "from funs import *\n",
    "\n",
    "# Colab\n",
    "import httpimport\n",
    "with httpimport.github_repo(\n",
    "    \"dmika1234\", \"ml_uwr_22\", module=\"Project\", branch=\"Project\"\n",
    "):\n",
    "    from Project.funs import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally\n",
    "# data_url = 'https://github.com/dmika1234/ml_uwr_22/blob/Project/Project/data/fake_job_postings.csv'\n",
    "data_path = 'data/fake_job_postings.csv'\n",
    "raw_data = pd.read_csv(data_path)\n",
    "\n",
    "# For colab\n",
    "# data_url = '/content/fake_job_postings.csv'\n",
    "# raw_data = pd.read_csv(data_url, error_bad_lines=False, engine=\"python\")\n",
    "#straszny problem miałem, żeby wczytać te dane tak ja ty to robiłeś. dziwne błędy mi wyskakiwały"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text_colnames = ['company_profile', 'description', 'requirements', 'benefits']\n",
    "DataPrep = DataPreprocessor()\n",
    "\n",
    "# text_data_ls = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=list)\n",
    "text_data_np = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=np.array)\n",
    "text_data_str = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=join_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting location into country, state, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df[['country', 'state', 'city']] = working_df['location'].str.split(',', expand=True).iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting salary range into min, max salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df[['salary_min', 'salary_max']] = working_df['salary_range'].str.split('-', expand=True)\n",
    "working_df[['salary_min', 'salary_max']] = working_df[['salary_min', 'salary_max']].apply(pd.to_numeric, errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['has_questions', 'has_company_logo', 'telecommuting', 'salary_min', 'salary_max']\n",
      "['company_profile', 'description', 'requirements', 'benefits']\n",
      "['required_education', 'state', 'function', 'country', 'required_experience', 'city', 'title', 'employment_type', 'industry', 'department']\n"
     ]
    }
   ],
   "source": [
    "target_colname = 'fraudulent'\n",
    "# Getting numerical colnames and deleting not useful\n",
    "numerical_colnames = list(working_df.select_dtypes(include='int64').columns)\n",
    "numerical_colnames = list(set(numerical_colnames) - set(['job_id', target_colname]))\n",
    "numerical_colnames = numerical_colnames + ['salary_min', 'salary_max']\n",
    "# Getting other text colnames and deleting not useful\n",
    "other_text_colnames = list(set(working_df.select_dtypes(include='object').columns) - set(text_colnames))\n",
    "other_text_colnames = list(set(other_text_colnames) - set(['location', 'salary_range']))\n",
    "print(numerical_colnames)\n",
    "print(text_colnames)\n",
    "print(other_text_colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df[text_colnames + other_text_colnames] = working_df[text_colnames + other_text_colnames].fillna('')\n",
    "working_df[numerical_colnames] = working_df[numerical_colnames].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employment_type            6\n",
       "required_experience        8\n",
       "required_education        14\n",
       "function                  38\n",
       "country                   91\n",
       "industry                 132\n",
       "state                    326\n",
       "department              1338\n",
       "city                    2336\n",
       "title                  11231\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df[other_text_colnames].apply(lambda x: np.unique(x).shape[0]).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use those with not so much levels(<50 for start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_other_text_colnames = ['employment_type', 'required_experience', 'required_education', 'function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(working_df[final_other_text_colnames], columns=final_other_text_colnames)\n",
    "X[numerical_colnames] = working_df[numerical_colnames]\n",
    "y = working_df[target_colname]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indcs, test_indcs = get_train_test_indcs(raw_data, raw_data['fraudulent'],\n",
    " test_size=.1, random_state=42, stratify=raw_data['fraudulent'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagofWords:\n",
    "    def __init__(self, text_data, text_colnames) -> None:\n",
    "        self.text_data = text_data\n",
    "        self.text_colnames = text_colnames\n",
    "        self.all_words = {}\n",
    "        self.most_popular_words = {}\n",
    "        self.nr_of_words = 1000\n",
    "        self.onehot_dfs = {}\n",
    "    def get_all_words(self):\n",
    "        for column_name in self.text_colnames:\n",
    "            self.all_words[column_name] = np.concatenate(self.text_data[column_name])\n",
    "\n",
    "    def get_most_pop_words(self, nr_of_words = 1000):\n",
    "        self.nr_of_words = nr_of_words\n",
    "        for column_name in self.text_colnames:\n",
    "            count_words = Counter(self.all_words[column_name])\n",
    "            self.most_popular_words[column_name] = np.array(count_words.most_common(self.nr_of_words))[:,0]\n",
    "    \n",
    "    def prepare_onehot_dfs(self):\n",
    "        for text_colname in self.text_colnames:\n",
    "            onehot_array = np.zeros((self.text_data.shape[0], self.nr_of_words))\n",
    "            for i in np.arange(self.text_data.shape[0]):\n",
    "                onehot_array[i] = np.isin(self.most_popular_words[text_colname], self.text_data[text_colname][i]).astype('int64')\n",
    "            column_names = np.vectorize(lambda x: text_colname + '_' + str(x))(self.most_popular_words[text_colname])\n",
    "            self.onehot_dfs[text_colname] = pd.DataFrame(onehot_array, columns=column_names)\n",
    "\n",
    "    def get_onehot_dfs(self):\n",
    "        X = pd.DataFrame()\n",
    "        for text_colname in self.text_colnames:\n",
    "            X = pd.concat((X, self.onehot_dfs[text_colname]), axis=1)\n",
    "        return X\n",
    "    \n",
    "    def encode_onehot(self, nr_of_words = 1000):\n",
    "        self.get_all_words()\n",
    "        self.get_most_pop_words(nr_of_words = 1000)\n",
    "        self.prepare_onehot_dfs()\n",
    "        X = self.get_onehot_dfs()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BagofWords(text_data_np, text_colnames)\n",
    "X_bow = bow.encode_onehot(1000)\n",
    "X_bow_final = pd.concat((X, X_bow), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TfidVectorizer to change text to numerical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_profile data successfuly transformed!\n",
      "description data successfuly transformed!\n",
      "requirements data successfuly transformed!\n",
      "benefits data successfuly transformed!\n"
     ]
    }
   ],
   "source": [
    "TfTrans = TfidTranformer(50)\n",
    "X_tfdif = TfTrans.vectorize_transform(text_data_str, train_indcs, text_colnames)\n",
    "X_tfdif_final = pd.concat((X, X_tfdif), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf_train, X_tf_test, y_train, y_test = X_tfdif_final.iloc[train_indcs], X_tfdif_final.iloc[test_indcs], y[train_indcs], y[test_indcs]\n",
    "X_bow_train, X_bow_test, y_train, y_test = X_bow_final.iloc[train_indcs], X_bow_final.iloc[test_indcs], y[train_indcs], y[test_indcs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(random_state = 2137,\n",
    "                                  class_weight = None)\n",
    "\n",
    "clf_tree = clf_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = clf_tree.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_percentage': 0.7471,\n",
       " 'precision': 0.7471,\n",
       " 'accuracy': 0.9754,\n",
       " 'f1_score': 0.7471,\n",
       " 'auc_roc': 0.8671}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tree = evaluate_performance(y_test, y_pred_tree) #None\n",
    "res_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_percentage': 0.7471,\n",
       " 'precision': 0.7471,\n",
       " 'accuracy': 0.9754,\n",
       " 'f1_score': 0.7471,\n",
       " 'auc_roc': 0.8671}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tree = evaluate_performance(y_test, y_pred_tree) #balanced\n",
    "res_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59,\n",
       " 299,\n",
       " {'ccp_alpha': 0.0,\n",
       "  'class_weight': None,\n",
       "  'criterion': 'gini',\n",
       "  'max_depth': None,\n",
       "  'max_features': None,\n",
       "  'max_leaf_nodes': None,\n",
       "  'min_impurity_decrease': 0.0,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_samples_split': 2,\n",
       "  'min_weight_fraction_leaf': 0.0,\n",
       "  'random_state': 1024,\n",
       "  'splitter': 'best'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf_tree.get_depth(), clf_tree.get_n_leaves(), clf_tree.get_params()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95134228, 0.96308725, 0.95917226, 0.95302013, 0.96420582,\n",
       "       0.95973154, 0.95805369, 0.96308725, 0.96420582, 0.96420582])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf_tree_cv = DecisionTreeClassifier(random_state = 2137,\n",
    "                                  class_weight = None)\n",
    "\n",
    "cross_val_score(clf_tree_cv, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_forest = RandomForestClassifier(random_state = 2137, \n",
    "                                    n_estimators = 200,\n",
    "                                    class_weight = \"balanced_subsample\")\n",
    "\n",
    "clf_forest = clf_forest.fit(X_train,y_train)\n",
    "\n",
    "y_pred_forest = clf_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_percentage': 0.5747,\n",
       " 'precision': 0.9615,\n",
       " 'accuracy': 0.9782,\n",
       " 'f1_score': 0.7194,\n",
       " 'auc_roc': 0.7868}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_forest = evaluate_performance(y_test, y_pred_forest) #None\n",
    "res_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_percentage': 0.5747,\n",
       " 'precision': 0.9615,\n",
       " 'accuracy': 0.9782,\n",
       " 'f1_score': 0.7194,\n",
       " 'auc_roc': 0.7868}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_forest = evaluate_performance(y_test, y_pred_forest) #balanced_subsample\n",
    "res_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_percentage': 0.6086956521739131,\n",
       " 'precision': 0.9767441860465116,\n",
       " 'accuracy': 0.9804332634521313,\n",
       " 'f1_score': 0.75,\n",
       " 'auc_roc': 0.8039807188916556}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_forest = evaluate_performance_tree(y_test, y_pred_forest) #balanced\n",
    "res_forest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fc72597e3ebe3b3cf4d5e7c9c96009b83c4c16e8afac821de511c3301abc35f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
