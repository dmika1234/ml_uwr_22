{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/dmika1234/ml_uwr_22/blob/Project/Project/tfidvec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'funs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhttpimport\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m httpimport\u001b[39m.\u001b[39mgithub_repo(\u001b[39m'\u001b[39m\u001b[39mdmika1234\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mml_uwr_22\u001b[39m\u001b[39m'\u001b[39m, branch\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProject\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m      \u001b[39mfrom\u001b[39;00m \u001b[39mfuns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'funs'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Funs\n",
    "# from funs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "def join_fun(list, sep=' '):\n",
    "    text = sep.join(list)\n",
    "    return text\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        nltk.download('stopwords') \n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def preprocess_data(self, text_data, column_names, vectorize_fun=list):\n",
    "        text_data = text_data[column_names].copy()\n",
    "        text_data.fillna('', inplace=True)\n",
    "        \n",
    "        for i in np.arange(text_data.shape[0]):\n",
    "            for col in column_names:\n",
    "                text_data.at[i,col] = self.tokenizer.tokenize(text_data.at[i,col].lower()) # removing punctuation\n",
    "                filtered_sentence = []\n",
    "                for w in text_data.at[i,col]:\n",
    "                    if w not in self.stop_words: # removing words such as “the”, “a”, “an”, “in”\n",
    "                        filtered_sentence.append(self.lemmatizer.lemmatize(w)) # grouping together the different inflected forms of a word \n",
    "                text_data.at[i,col] = vectorize_fun(filtered_sentence)\n",
    "\n",
    "        return text_data\n",
    "\n",
    "\n",
    "def get_train_test_indcs(X, y, test_size, random_state, stratify):\n",
    "    X_train, X_test = train_test_split(X, y, test_size=test_size,\n",
    "     random_state=random_state, stratify=stratify)[0:2]\n",
    "    return X_train.index, X_test.index\n",
    "\n",
    "\n",
    "def evaluate_performance(y_test, y_pred, prob=False, threshold=0.2, round=4, save_pred=False):\n",
    "    if prob:\n",
    "        y_pred = (y_pred >= threshold).astype('int64')\n",
    "    results = {}\n",
    "    if save_pred:\n",
    "        results['y_pred'] = y_pred\n",
    "    # Get the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    # Calculate the detection percentage\n",
    "    results['detection_percentage'] = np.round(tp / (tp + fn), round)\n",
    "    # Calculate precision\n",
    "    results['precision'] = np.round(tp / (tp + fp), round)\n",
    "    # Calculate accuracy\n",
    "    results['accuracy'] = np.round((tp + tn) / (tp + tn + fp + fn), round)\n",
    "    # Calculate F1-score\n",
    "    results['f1_score'] = np.round(2 * (results['precision'] * results['detection_percentage']) /\\\n",
    "                          (results['precision'] + results['detection_percentage']), round)\n",
    "    results['auc_roc'] = np.round(roc_auc_score(y_test, y_proba), round)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally\n",
    "# data_url = 'https://github.com/dmika1234/ml_uwr_22/blob/Project/Project/data/fake_job_postings.csv'\n",
    "data_path = 'data/fake_job_postings.csv'\n",
    "raw_data = pd.read_csv(data_path)\n",
    "\n",
    "# For colab\n",
    "# data_url = '/content/fake_job_postings.csv'\n",
    "# raw_data = pd.read_csv(data_url, error_bad_lines=False, engine=\"python\")\n",
    "#straszny problem miałem, żeby wczytać te dane tak ja ty to robiłeś. dziwne błędy mi wyskakiwały"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text_colnames = ['company_profile', 'description', 'requirements', 'benefits']\n",
    "DataPrep = DataPreprocessor()\n",
    "\n",
    "# text_data_ls = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=list)\n",
    "# text_data_np = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=np.array)\n",
    "text_data_str = DataPrep.preprocess_data(text_data=raw_data, column_names=text_colnames, vectorize_fun=join_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting location into country, state, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df[['country', 'state', 'city']] = working_df['location'].str.split(',', expand=True).iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting salary range into min, max salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df[['salary_min', 'salary_max']] = working_df['salary_range'].str.split('-', expand=True)\n",
    "working_df[['salary_min', 'salary_max']] = working_df[['salary_min', 'salary_max']].apply(pd.to_numeric, errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['has_questions', 'telecommuting', 'has_company_logo', 'salary_min', 'salary_max']\n",
      "['company_profile', 'description', 'requirements', 'benefits']\n",
      "['industry', 'required_experience', 'title', 'state', 'employment_type', 'required_education', 'city', 'department', 'country', 'function']\n"
     ]
    }
   ],
   "source": [
    "target_colname = 'fraudulent'\n",
    "# Getting numerical colnames and deleting not useful\n",
    "numerical_colnames = list(working_df.select_dtypes(include='int64').columns)\n",
    "numerical_colnames = list(set(numerical_colnames) - set(['job_id', target_colname]))\n",
    "numerical_colnames = numerical_colnames + ['salary_min', 'salary_max']\n",
    "# Getting other text colnames and deleting not useful\n",
    "other_text_colnames = list(set(working_df.select_dtypes(include='object').columns) - set(text_colnames))\n",
    "other_text_colnames = list(set(other_text_colnames) - set(['location', 'salary_range']))\n",
    "print(numerical_colnames)\n",
    "print(text_colnames)\n",
    "print(other_text_colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df[text_colnames + other_text_colnames] = working_df[text_colnames + other_text_colnames].fillna('')\n",
    "working_df[numerical_colnames] = working_df[numerical_colnames].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employment_type            6\n",
       "required_experience        8\n",
       "required_education        14\n",
       "function                  38\n",
       "country                   91\n",
       "industry                 132\n",
       "state                    326\n",
       "department              1338\n",
       "city                    2336\n",
       "title                  11231\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_df[other_text_colnames].apply(lambda x: np.unique(x).shape[0]).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use those with not so much levels(<50 for start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_other_text_colnames = ['employment_type', 'required_experience', 'required_education', 'function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(working_df[final_other_text_colnames], columns=final_other_text_colnames)\n",
    "X[numerical_colnames] = working_df[numerical_colnames]\n",
    "y = working_df[target_colname]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indcs, test_indcs = get_train_test_indcs(raw_data, raw_data['fraudulent'],\n",
    " test_size=.1, random_state=42, stratify=raw_data['fraudulent'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TfidVectorizer to change text to numerical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_profile data successfuly transformed!\n",
      "description data successfuly transformed!\n",
      "requirements data successfuly transformed!\n",
      "benefits data successfuly transformed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17880, 271)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "number_of_vars = 50\n",
    "X_tfdif = pd.DataFrame()\n",
    "for colname in text_colnames:\n",
    "    vectorizer = TfidfVectorizer(max_features=number_of_vars, min_df=2).fit(text_data_str[colname][train_indcs])\n",
    "    df_transformed = pd.DataFrame(vectorizer.transform(text_data_str[colname]).toarray(), \n",
    "    columns=['num_' + colname + '_' + str(nr) for nr in np.arange(number_of_vars)])\n",
    "    print(f'{colname} data successfuly transformed!')\n",
    "    X_tfdif = pd.concat((X_tfdif, df_transformed), axis=1)\n",
    "X_tfdif.shape\n",
    "X_tfdif_final = pd.concat((X, X_tfdif), axis=1)\n",
    "X_tfdif_final.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf_train, X_tf_test, y_train, y_test = X_tfdif_final.iloc[train_indcs], X_tfdif_final.iloc[test_indcs], y[train_indcs], y[test_indcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpimport\n",
    "with httpimport.github_repo('dmika1234', 'ml_uwr_22', \n",
    "                            module='Project', branch='Project'):\n",
    "     from funs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniversityEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15f8f2d3d77985f79729f955a1d3618c4763baf8483cab5a52cfb8bdd5f22be1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
